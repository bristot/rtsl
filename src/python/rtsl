#!/usr/bin/env python3
# SPDX-License-Identifier: GPL-2.0-only
#
# rtsl: more information, see:
#   https://bristot.me/demystifying-the-real-time-linux-latency/
#
# This program was written in the development of this paper:
#
# de Oliveira, D. B., Casini, D., de Oliveira, R. S., Cucinotta, T.
# "Demystifying the Real-Time Linux Scheduling Latency". 2020, In
# 32nd Euromicro Conference on Real-time Systems (ECRTS 2020).
#
# Copyright 2018-2020 Red Hat, Inc.
#
# Author:
#  Daniel Bristot de Oliveira <bristot@redhat.com>

import shlex
import shutil
import subprocess
import math
import sqlite3
import os

class RtslKernelModule:
    """
    This is the rtsl kernel module interface class.

    This class provides methods to enable and disable the
    rtsl in-kernel tracing features.
    """
    __rtsl_interface_dir="/sys/kernel/debug/rtsl/"

    def rtsl_check_interface(self):
        """
        Check if the rtsl is loaded by trying to open the enable interface.
        """

        enable_path=self.__rtsl_interface_dir + "enable"
        enable=open(enable_path, 'w')
        enable.close()

    def __rtsl_interface_enable(self, on_off):
        """
        Write in rtsl/enable file.
        """
        enable_path=self.__rtsl_interface_dir + "enable"

        try:
            enable=open(enable_path, 'w')
        except Exception as e:
            raise e

        try:
            enable.write(on_off)
        except:
            raise Exception("Cannot write to the RTSL interface: %s" % e.strerror)

        enable.close()

    def rtsl_enable(self):
        """
        Enable rtsl tracepoints.
        """
        self.__rtsl_interface_enable("1")

    def rtsl_disable(self):
        """
        Disable rtsl tracepoints.
        """
        self.__rtsl_interface_enable("0")

class RtslTrace:
    """
    Trace helper class.

    This class provides helper functions to call perf/ftrace tracing tools.

    The rtsl does not trace the system itself. Instead, it uses either perf
    or ftrace to collect and parse the events.

    The "record" mode calls the tracing tools to record the tracepoints
    provided by the rtsl kernel module (see RtslKernelModule class).

    The "report" mode calls the tracing tools to parse the trace, translating
    it into the database expected by the RtslDatabase class. The parsing is
    not done by perf/ftrace themselves, but by a trace plugin provided by the
    rtsl toolkit.

    The "report" mode is done in parallel, dispatching one (independent) tracer
    instance per CPU found in the trace file. Each tracer instance will crate
    one database named trace_${CPUID}.rtsl.
    """

    __modes={ "record" : 1, "report" : 2, }
    __mode=None

    __backends={ "perf" : 1, "ftrace" : 2, }
    __backend=None

    __run_cyclictest=False
    __cyclictest_parameters="-q"

    __verbose=False

    __rtsl_events=[ "rtsl:max_paie",
                    "rtsl:max_poid",
                    "rtsl:max_dst",
                    "rtsl:max_psd",
                    "rtsl:irq_execution",
                    "rtsl:nmi_execution" ]

    __trace_cmd_plugins=["/usr/lib64/trace-cmd/plugins/", "~/.trace-cmd/plugins/" ]
    __perf_plugins=["/usr/lib64/trace-cmd/plugins/", "~/.local/lib/traceevent/plugins/" ]

    __working_dir="rtsl_data"
    __trace_file=""
    __module=None

    def __init__(self, mode, duration=60, backend="perf", cyclictest=False, cyclictest_parameters="", verbose=False, working_dir="rtsl_data"):
        """
        The "record" mode uses the "backend" tool to record trace, placing
        the output in the "working_dir." The trace tool runs for "duration"
        in seconds.

        The "report" mode checks the trace file in the "working_dir," calling
        the respective "backend" tool. The "report" mode depends on a plugin
        provided by rtsl, that must exist in the self.__trace_cmd_plugins or
        self.__perf_plugins directory.
        """

        self.__mode=self.__modes.get(mode)
        if self.__mode == None:
            raise Exception("Unknown mode: %s" % mode)

        if self.__backends.get(backend) == None:
            raise Exception("Unknown backend: %s" % mode)

        self.__backend = backend

        if mode == "record":
            # try to read the module
            self.__module=RtslKernelModule()
            try:
                self.__module.rtsl_check_interface()
            except Exception as e:
                raise Exception("Cannot access RTSL interface: %s" % e.strerror)

            self.duration = duration

            if cyclictest == True:
                self.__run_cyclictest = true
                self.__cyclictest_parameters += cyclictest_parameters
                self.__cyclictest_parameters += "-d%ds" % (duration)
                print(self.__cyclictest_parameters)

        if verbose:
            self.__verbose=True
        self.__working_dir=working_dir

    def __prepare_working_dir(self):
        """
        Creates the working_dir, renaming a previously existing
        dir to a "working_dir.old.
        """
        if os.path.exists(self.__working_dir):
            old_working_dir = "%s.old" % self.__working_dir

            if os.path.exists(old_working_dir):
                try:
                    shutil.rmtree(old_working_dir)
                except:
                    raise Exception("Fail to remove the old working dir (%s)" % old_working_dir)

            try:
                os.rename(self.__working_dir, old_working_dir)
            except:
                raise Exception("Old working directory exist, but it cannot be moved.")

        try:
            os.mkdir(self.__working_dir)
        except:
            raise Exception("Cannot create the working dir.")

    def __get_file_multipath(self, paths, file_name):
        """
        Find a file on multiple paths.
        """
        for path in paths:
            file_path=os.path.expanduser(path) + "/" +  file_name
            if os.path.exists(file_path):
                return file_path

        return None

    def __run_command(self, command, silent=False):
        """
        Run a command, returning the process descriptor.
        """
        if silent:
            output=subprocess.DEVNULL
        else:
            output=subprocess.PIPE

        try:
            process = subprocess.Popen(shlex.split(command), stdout=output, stderr=subprocess.PIPE)
        except Exception as e:
            raise e

        return process

    def __poll_process_output(self, process, save_output=False, silent=False):
        """
        Poll a process output until the process ends.
        ^C terminates the process, ^C again kills it.
        """
        saved_output=[]
        term_sent=False
        while True:
            try:
                if silent:
                    output = process.stderr.readline()
                else:
                    output = process.stdout.readline()
            except KeyboardInterrupt:
                if not term_sent:
                    print("\nKeyboard interrupt, sending SIGTERM...")
                    process.terminate()
                    term_sent = True
                else:
                    print("\nKeyboard interrupt again, ok, got it, killing...")
                    process.kill()
            except Exception as e:
                raise e

            if self.__verbose:
                print(output.decode(), end="")

            if save_output:
                saved_output.append(output.decode())

            rc = process.poll()
            if rc is not None:
                break

        return saved_output

    # -------------------- report --------------------
    def __record_tracer_cmd(self, tracer_cmd_line):
        """
        Run a tracer in the record mode, enabling and disabling rtsl
        interface accordingly.
        """

        # Try to run it.
        try:
            trace_cmd = self.__run_command(tracer_cmd_line)
        except Exception as e:
            raise e

        # Let the tracer to start up.
        time.sleep(2)

        # If it runs, enable rtsl events. This includes hooking to
        # the tracepoints and start the values of the thread windows.
        try:
            self.__module.rtsl_enable()
        except Exception as e:
            trace_cmd.kill()
            raise Exception("Error enabling RTSL: %s" % e.strerror)

        # So far so good, wait for the tracer to return.
        self.__poll_process_output(trace_cmd)

        # Disable the rtsl (unhook the tracepoints)
        try:
            self.__module.rtsl_disable()
        except Exception as e:
            raise Exception("Error disabling RTSL: %s" % e.strerror)

        # ta da!

    def __record_ftrace(self):
        """
        trace-cmd record: record the rtsl events.
        """

        # Create the trace-cmd string.
        event_cmd=""
        for event in self.__rtsl_events:
            event_cmd += "-e %s " % event

        command="trace-cmd record -r1 -o %s %s sleep %d" % (self.__trace_file, event_cmd, self.duration)
        self.__record_tracer_cmd(command)

    def __record_perf(self):
        """
        perf record: record the rtsl events.
        """

        # Create the perf string.
        event_cmd=""
        for event in self.__rtsl_events:
            event_cmd += "-e %s " % event

        command="perf record -o %s -a %s sleep %d" % (self.__trace_file, event_cmd, self.duration)
        self.__record_tracer_cmd(command)

    def record(self):
        """
        record: use trace to record the variables exported by the rtsl tracer.
        """

        self.__prepare_working_dir()

        try:
            if self.__backend == "ftrace":
                self.__trace_file="%s/%s" % (self.__working_dir, "trace.dat")
                self.__record_ftrace()
            elif self.__backend == "perf":
                self.__trace_file="%s/%s" % (self.__working_dir, "perf.data")
                self.__record_perf()
            else:
                print("Unknown backend...")
                return
        except Exception as e:
            print(str(e))

    # -------------------- report --------------------
    def __report_tracer_cmd(self, tracer_cmd_line, cpu_list, reparse=False):
        """
        Run the report command, instructing the tracing plugin about the
        database name.
        """
        process_list=[]
        db_name_list=[]

        # Tell the plugin to record the database.
        os.environ["RTSL_RECORD_DB"] = "1"

        for cpu in cpu_list:
            db_name = self.__working_dir + "/trace_" + cpu + ".rtsl"

            db_name_list.append(db_name)

            if reparse == False:
                if os.path.exists(db_name):
                    continue

            os.environ["RTSL_DB_NAME"] = db_name

            # Try to run the conversion
            per_cpu_cmd_line = tracer_cmd_line + " --cpu " + cpu
            process = self.__run_command(per_cpu_cmd_line, silent=True)

            process_list.append(process)

        # Wait
        for process in process_list:
            self.__poll_process_output(process, silent=True)

        os.environ.pop("RTSL_RECORD_DB")

    def __report_ftrace_get_cpus(self):
        """
        Returns the list of CPUs of the system that was traced with ftrace.
        """
        command="trace-cmd report --stat -i %s" % self.__trace_file
        process = self.__run_command(command)
        output = self.__poll_process_output(process, save_output=True)

        cpus=-1
        cpu_list=[]
        for i in output:
            # trace-cmd reports:
            # cpus=NR_CPUS
            if i[0:5] == "cpus=":
                cpus=i.replace("=", " ").split()[1]

            # trace-cmd reports:
            # for each CPU
            #     CPU: CPU_ID
            if i[0:4] == "CPU:":
                cpu_list.append(i.split()[1])

        return cpu_list

    def __report_ftrace(self, params):
        """
        trace-cmd report: parse trace.dat converting into the sqlite database.
        """

        # Check if the trace-cmd plugin that converts the trace.dat into a
        # database exists before start.
        if self.__get_file_multipath(self.__trace_cmd_plugins, "plugin_rtsl.so") is None:
            raise Exception("trace-cmd plugin is not installed, cannot proceed.")

        cpu_list = self.__report_ftrace_get_cpus()

        command="trace-cmd report -i %s" % self.__trace_file

        self.__report_tracer_cmd(command, cpu_list, reparse=params.reparse)

    def __report_perf_get_cpus(self):
        """
        Returns the list of CPUs of the system that was traced with perf.
        """
        command="perf report -i %s --header-only -I" % self.__trace_file
        process = self.__run_command(command)
        output = self.__poll_process_output(process, save_output=True)

        cpus=-1
        cpu_list=[]
        for i in output:
            # perf reports:
            # '# nrcpus avail : 8' (yes, there is a # at the beginning)
            if i[0:14] == "# nrcpus avail":
                cpus=i.replace("=", " ").split()[4]

            # perf reports
            # for each CPU
            #     '# CPU 0: Core ID 0, Die ID 0, Socket ID 0"
            # we only acre about the CPU.
            if i[0:6] == "# CPU ":
                cpu=i.split()[2].replace(":","")
                if cpu.isdigit():
                    cpu_list.append(cpu)

        return cpu_list

    def __report_perf(self, params):
        """
        perf report: parse perf.data converting into the sqlite database.
        """

        # Check if the perf plugin that converts the perf.data into a database
        # exists before start.
        if self.__get_file_multipath(self.__perf_plugins, "plugin_rtsl.so") is None:
            raise Exception("perf plugin is not installed, cannot proceed.")

        cpu_list = self.__report_perf_get_cpus()

        command="perf script -i %s" % self.__trace_file
        self.__report_tracer_cmd(command, cpu_list, reparse=params.reparse)

    def report(self, params):
        """
        Convert a trace file (trace.dat/perf.data) into the sqlite database,
        then run the analysis on it.
        """

        if os.path.exists(self.__working_dir + "/trace.dat"):
            self.__trace_file="%s/%s" % (self.__working_dir, "trace.dat")
            self.__report_ftrace(params)
        elif os.path.exists(self.__working_dir + "/perf.data"):
            self.__trace_file="%s/%s" % (self.__working_dir, "perf.data")
            self.__report_perf(params)

        else:
            raise Exception("Did not find a trace file in the working dir (%s)" % self.__working_dir)

class RtslDatabase:
    """
    This is a "per-database" representation of the systems, aiming to produce
    the per-cpu latency analysis for all CPUs present in a database.

    The rtsl tool keeps one CPU per database, in such a way to enable the
    execution of one thread analysis per CPU, which is a good heuristic,
    assuming that the trace and the report will be done on the same system
    (fully parallelized).

    But it might as well be the case that we decided to merge multiple databases
    into a single database. At this point, I am not sure what will happen so
    that I will keep things as they are, removing the CPU field in the future if
    we decide to keep only one CPU per database.
    """

    conn=None
    infinito=999999999

    def __init__(self, database_file="trace.rtsl", reset_results=False):
        """
        Open a database, resetting the results if requested.
        """
        if os.path.exists(database_file) == False:
            raise Exception("Database %s does not exist" % database_file)

        self.__conn_database(database_file)

        if reset_results:
            self.create_database_results()

    def __conn_database(self, database_file="trace.rtsl"):
        """
        Connect to a database, with some tune.
        """
        self.conn = sqlite3.connect(database_file, isolation_level=None, check_same_thread=False)
        self.conn.execute('pragma journal_mode = wal')
        self.conn.execute('pragma synchronous = 0')
        self.conn.execute('pragma cache_size = -200000')

    def create_database_cache(self):
        """
        Create a cache of commonly used values, to speed up the analysis.
        """
        c = self.conn.cursor()
        c.execute('''DROP TABLE IF EXISTS cache_nmi_owcet''')
        c.execute('''DROP TABLE IF EXISTS cache_nmi_omiat''')
        c.execute('''DROP TABLE IF EXISTS cache_irq_owcet''')
        c.execute('''DROP TABLE IF EXISTS cache_irq_omiat''')

        c.execute('''CREATE TABLE cache_nmi_owcet (cpu integer PRIMARY KEY, duration integer)''')
        c.execute('''CREATE TABLE cache_irq_owcet (cpu integer, vector integer, duration integer, PRIMARY KEY(cpu, vector))''')
        c.execute('''CREATE TABLE cache_nmi_omiat (cpu integer PRIMARY KEY, miat integer)''')
        c.execute('''CREATE TABLE cache_irq_omiat (cpu integer, vector integer, miat integer, PRIMARY KEY(cpu, vector))''')

        cpus=self.get_cpu_list()

        for cpu in cpus:
            owcet = self.__get_nmi_cpu_owcet(cpu)
            c.execute("INSERT OR REPLACE INTO cache_nmi_owcet VALUES (?, ?)", (cpu, owcet))

            miat = self.__get_nmi_cpu_miat(cpu)
            c.execute("INSERT OR REPLACE INTO cache_nmi_omiat VALUES (?, ?)", (cpu, miat))

            vectors = self.get_irq_vectors_of_a_cpu(cpu)
            for vector in vectors:
                owcet = self.__get_irq_vector_owcet(cpu, vector)
                miat = self.__get_irq_vector_miat(cpu, vector)

                c.execute("INSERT OR REPLACE INTO cache_irq_owcet VALUES (?, ?, ?)", (cpu, vector, owcet))
                c.execute("INSERT OR REPLACE INTO cache_irq_omiat VALUES (?, ?, ?)", (cpu, vector, miat))
        self.conn.commit()

    def create_database_results(self):
        """
        Create the tables that store the results.
        """
        c = self.conn.cursor()
        c.execute('''DROP TABLE IF EXISTS analysis_output''')
        c.execute('''DROP TABLE IF EXISTS results''')

        c.execute('''CREATE TABLE analysis_output (cpu integer, line text)''')
        c.execute('''CREATE TABLE results (cpu integer, name TEXT, latency integer, PRIMARY KEY(cpu, name))''')
        self.conn.commit()

    # -------------------- CPU gets --------------------
    def get_cpu_list(self):
        """
        Returns the list[] of CPUs in the database, based on the CPU id's that
        reported passing by a POID.
        """
        cpu_list=[]
        for entry in self.conn.execute("SELECT DISTINCT(cpu) from poid"):
            cpu_list.append(entry[0])
        cpu_list.sort()
        return cpu_list

    # -------------------- Thread variable GETs --------------------
    def get_poid(self, cpu):
        """
        Returns the POID of a given CPU
        """
        try:
            return self.conn.execute("SELECT * from poid WHERE cpu=?", (cpu,)).fetchone()[1]
        except:
            return 0

    def get_paie(self, cpu):
        """
        Returns the PAIE of a given CPU
        """
        try:
            return self.conn.execute("SELECT * from paie WHERE cpu=?", (cpu,)).fetchone()[1]
        except:
            return 0

    def get_psd(self, cpu):
        """
        Returns the PSD of a given CPU
        """
        try:
            return self.conn.execute("SELECT value from psd WHERE cpu=?", (cpu,)).fetchone()[0]
        except:
            return 0

    def get_dst(self, cpu):
        """
        Returns the DST of a given CPU
        """
        try:
            return self.conn.execute("SELECT * from dst WHERE cpu=?", (cpu,)).fetchone()[1]
        except:
            return 0

    # -------------------- IRQ GETs --------------------
    def get_irq_cpu_owcet(self, cpu):
        """
        Returns the duration of the longest IRQ execution of a given CPU.
        """
        return self.conn.execute("select MAX(duration) from irq where cpu=?", (cpu,)).fetchone()[0]

    def get_irq_vectors_of_a_cpu(self, cpu):
        """
        Return the list[] of IRQ entries of a given CPU
        """

        vector_list=[]

        for vector in self.conn.execute("select DISTINCT(vector) from irq where cpu=?", (cpu,)):
            vector_list.append(vector[0])

        vector_list.sort()

        return vector_list

    def get_irq_entries_of_a_vector(self, cpu, vector):
        """
        Returns a list[] with all IRQ occurrences of a given vector of a given CPU.

        Each element contains a tuple() with the start_time and duration of the
        IRQ occurrence.

        This function consumes lots of memory, but speeds up things.

        XXX: X Needs to create a version to be used by low memory systems.
        """

        irq_list=[]

        for irq in self.conn.execute("SELECT start_time, duration from irq WHERE cpu=? AND vector=?", (cpu, vector)):
            irq_list.append(irq)

        return irq_list

    def __get_irq_vector_owcet(self, cpu, vector):
        """
        Returns the worst execution time of a given IRQ on a given CPU.
        """

        return self.conn.execute("select MAX(duration) from irq where cpu=? and vector=?", (cpu, vector)).fetchone()[0]

    def get_irq_vector_owcet(self, cpu, vector):
        """
        Returns the worst execution time of a given IRQ on a given CPU.

        But it tries to check the cache first.
        """

        try:
            return self.conn.execute("select duration from cache_irq_owcet where cpu=? and vector=?", (cpu,vector)).fetchone()[0]
        except:
            return self.__get_irq_vector_owcet(cpu, vector)

    def __get_irq_vector_miat(self, cpu, vector):
        """
        Returns the minimum inter-arrival time of a given IRQ on a given CPU.

        The minimum inter-arrival time is the shortest delta between two IRQ occurrence.
        """

        min_iat=self.infinito
        last_arrival=0

        for irq in self.conn.execute("SELECT start_time from irq WHERE cpu=? AND vector=?", (cpu, vector)):
            curr_irq=irq[0]

            if last_arrival:
                iat=curr_irq - last_arrival
                if iat < min_iat:
                    min_iat=iat

            last_arrival=curr_irq

        return min_iat

    def get_irq_vector_miat(self, cpu, vector):
        """
        Returns the minimum inter-arrival time of a given IRQ on a given CPU.

        But it tries to check the cache first.
        """

        try:
            return self.conn.execute("select miat from cache_irq_omiat where cpu=? and vector=?", (cpu,vector)).fetchone()[0]
        except:
            return self.__get_irq_vector_miat(cpu, vector)

    # -------------------- IRQ GETs --------------------
    def get_nmi_cpu_miat(self, cpu):
        """
        Returns the duration of the longest NMI execution of a given CPU.
        """
        try:
            return self.conn.execute("select miat from cache_nmi_omiat where cpu=?", (cpu,)).fetchone()[0]
        except:
            return self.__get_nmi_cpu_owcet(cpu)

    def get_nmi_entries_of_cpu(self, cpu):
        """
        Returns a list[] with all NMI occurrences of a given CPU.

        Each element contains a tuple() with the start_time and duration of the
        NMI occurrence.

        This function consumes lots of memory, but speeds up things.

        XXX: X Needs to create a version to be used by low memory systems.
        """
        nmi_list=[]

        for nmi in self.conn.execute("SELECT start_time, duration from nmi WHERE cpu=?", (cpu,)):
            nmi_list.append(nmi)

        return nmi_list

    def __get_nmi_cpu_owcet(self, cpu):
        """
        Returns the worst execution time of a NMI on a given CPU.
        """
        owcet = self.conn.execute("select MAX(duration) from nmi where cpu=?", (cpu,)).fetchone()[0]
        try:
            return int(owcet)
        except:
            return 0

    def get_nmi_cpu_owcet(self, cpu):
        """
        Returns the worst execution time of a NMI on a given CPU.
        """
        try:
            return self.conn.execute("select duration from cache_nmi_owcet where cpu=?", (cpu,)).fetchone()[0]
        except:
            return self.__get_nmi_cpu_owcet(cpu)

    def __get_nmi_cpu_miat(self, cpu):
        """
        Returns the minimum inter-arrival time of a NMI on a given CPU.

        The minimum inter-arrival time is the shortest delta between two NMI occurrence.
        """
        min_iat=self.infinito
        last_arrival=0
        for nmi in self.conn.execute("SELECT start_time from nmi WHERE cpu=?", (cpu,)):
            curr_nmi=nmi[0]
            if last_arrival:
                iat=curr_nmi - last_arrival
                if iat < min_iat:
                    min_iat=iat
            last_arrival=curr_nmi

        return min_iat

    # -------------------- get/set results --------------------
    def insert_results(self, cpu, name, latency):
        """
        Insert the latency result of an analysis.
        """
        self.conn.execute("INSERT OR REPLACE INTO results VALUES (?, ?, ?)", (cpu, name, latency))

    def get_results(self):
        """
        Get the latency results of all analysis"
        """
        results=[]
        for result in self.conn.execute("SELECT cpu, name, latency from results"):
            results.append(result)

        return results

    def insert_analysis_output(self, cpu, line):
        """
        Insert the text output from the analysis into the database.
        """
        self.conn.execute("INSERT INTO analysis_output VALUES (?, ?)", (cpu, line))

    def get_analysis_output(self):
        """
        Returns all the lines from the previously processed analysis.
        """
        results=[]

        for line in self.conn.execute("SELECT line from analysis_output"):
            results.append(line[0])

        return results

class RtslINTAnalysis(RtslDatabase):
    """
    This class provides methods to analyse the IRQ occurrence present in a
    RtslDatabase.

    Results are saved to the same database.
    """

    has_cyclictest_results=False
    __cyclictest_data=[]

    std_output={}

    def __init__(self, database_file="trace.rtsl"):
        """
        Open the database.
        """
        super().__init__(database_file, reset_results=True)

    def print_cpu_analysis(self, cpu, line):
        """
        Adds a line of analysis to the per-cpu analysis.
        """
        self.insert_analysis_output(cpu, line)

    def __save_results(self, cpu, latency, case):
        self.insert_results(cpu, case, latency)

    def print_latency(self, cpu, latency, case):
        """
        Adds a line with the results of the analysis.
        """
        line="\t\tLatency = %9d with %s" % (latency, case)

        self.insert_analysis_output(cpu, line)
        self.__save_results(cpu, latency, case)

    # -------------------- Analysis --------------------
    def open_cyclictest(self, file_path="cyclictest.txt"):
        """
        Open cyclictest output.

        The output must be generated with "-q" option.
        """
        try:
            cyclic_file = open(file_path)
        except OSError:
            return False

        self.__cyclictest_data = cyclic_file.read().splitlines()
        cyclic_file.close()
        return True

    def print_cyclictest(self, cpu):
        """
        Returns the cyclictest's latency of a CPU.
        """

        self.print_cpu_analysis(cpu, "\tCyclictest:")

        for line in self.__cyclictest_data:
            vector=line[2:].split()
            try:
                cpu_vector=int(vector[0])
            except:
                continue

            if cpu_vector == cpu:
                self.print_latency(cpu, int(vector[-1])*1000, "Cyclictest")

                return int(vector[-1])*1000, "Cyclictest"

    def interference_free_latency(self, cpu, poid, paie, psd, dst):
        """
        Returns the interference free latency of a given CPU.
        """
        self.print_cpu_analysis(cpu, "\tInterference Free Latency:")
        self.print_cpu_analysis(cpu, "\t\t  latency = max(     poid,       dst) +      paie +       psd")

        latency = max(poid, dst) + paie + psd

        self.print_cpu_analysis(cpu, "\t\t%9d = max(%9d, %9d) + %9d + %9d" % (latency, poid, dst, paie, psd))

        return latency

    def no_interrupt(self, cpu, ifl):
        """
        Latency considering only the threads of a given CPU.
        """
        self.print_cpu_analysis(cpu, "\tNo interrupts:")
        self.print_latency(cpu, ifl, "No Interrupts")

        return ifl, "No Interrupts"

    def single_interrupt(self, cpu, ifl):
        """
        Latency considering a single NMI and a single IRQ (the worst).
        """
        max_irq=0
        max_nmi=0

        self.print_cpu_analysis(cpu, "\tConsidering a single NMI and IRQ (the worst):")
        self.print_cpu_analysis(cpu, "\t\t  latency =       ifl +       IRQ +      NMI")

        max_irq = self.get_irq_cpu_owcet(cpu)
        max_nmi = self.get_nmi_cpu_owcet(cpu)

        latency = ifl + max_irq + max_nmi

        self.print_cpu_analysis(cpu, "\t\t%9d = %9d + %9d +%9d" % (latency, ifl, max_irq, max_nmi))
        self.print_latency(cpu, latency, "Worst Single Interrupt")

        return latency, "Worst Single Interrupt"

    def single_of_each_interrupt(self, cpu, ifl):
        """
        Latency considering a single NMI and the sum of one occurrence of each
        Interrupt (the worst).
        """
        self.print_cpu_analysis(cpu, "\tConsidering a single NMI and the sum of one occurrence of each Interrupt (the worst):")

        max_nmi=0
        max_irq=0
        sum_irq=0

        vector_list=self.get_irq_vectors_of_a_cpu(cpu)

        self.print_cpu_analysis(cpu, "\t\t%3s: %9s" % ("INT", "oWCET"))

        max_nmi = self.get_nmi_cpu_owcet(cpu)
        self.print_cpu_analysis(cpu, "\t\tNMI: %9d" % (max_nmi))

        for vector in vector_list:
            max_irq = self.get_irq_vector_owcet(cpu, vector)
            self.print_cpu_analysis(cpu, "\t\t%3d: %9d" % (vector, max_irq))
            sum_irq += max_irq

        latency = ifl + sum_irq + max_nmi

        self.print_cpu_analysis(cpu, "\t\t  latency =       ifl +  sum(IRQ) +      NMI")

        self.print_cpu_analysis(cpu, "\t\t%9d = %9d + %9d +%9d" % (latency, ifl, sum_irq, max_nmi))

        self.print_latency(cpu, latency, "Single (Worst) of Each Interrupt")

        return latency, "Single (Worst) of Each Interrupt"

    def __rta_irq_interference(self, window, irq_param):
        """
        A round of RTA.
        """
        sum_irq=0
        for vector in irq_param:
            c, t = irq_param[vector]

            # if no NMI...
            if t == 0:
                continue

            irq_uw = math.ceil(window/t)*c
            sum_irq += irq_uw
        return sum_irq

    def sporadic_interrupt(self, cpu, ifl):
        """
        Latency considering sporadic interrupt.
        """
        self.print_cpu_analysis(cpu, "\tConsidering sporadic interrupts:")
        wont_converge=0
        irq_param={}
        vector_list=self.get_irq_vectors_of_a_cpu(cpu)

        self.print_cpu_analysis(cpu, "\t\t%3s: %9s %15s" % ("INT", "oWCET", "oMIAT"))

        # NMI
        try:
            wcet = self.get_nmi_cpu_owcet(cpu)
            miat = self.get_nmi_cpu_miat(cpu)
        except:
            wcet = miat = 0

        irq_param['NMI'] = wcet, miat

        self.print_cpu_analysis(cpu, "\t\t%3s: %9d %15d" % ('NMI', wcet, miat))

        for vector in vector_list:
            wcet = self.get_irq_vector_owcet(cpu, vector)
            miat = self.get_irq_vector_miat(cpu, vector)

            if miat < wcet:
                self.print_cpu_analysis(cpu, "\t\t%3d: %9d %15d <- oWCET > oMIAT: won't converge" % (vector, wcet, miat))
                wont_converge=1
            else:
                if miat == self.infinito:
                    self.print_cpu_analysis(cpu, "\t\t%3d: %9d %15s" % (vector, wcet, "infinite"))
                else:
                    self.print_cpu_analysis(cpu, "\t\t%3d: %9d %15d" % (vector, wcet, miat))
            irq_param[vector] = wcet, miat

        if wont_converge:
            self.print_cpu_analysis(cpu, "\t\tDid not converge.")
            self.print_latency(cpu, 0, "Sporadic")
            return

        c_latency=ifl
        window=c_latency
        self.print_cpu_analysis(cpu, "\t\t%9s: %9s" % ("initial w", "ifl"))
        self.print_cpu_analysis(cpu, "\t\t%9d: %9d" % (window, ifl))

        while True:
            new_window  = c_latency + self.__rta_irq_interference(window, irq_param)
            self.print_cpu_analysis(cpu, "\t\t%9s= %d" % ("w'", new_window))

            if new_window == window:
                self.print_cpu_analysis(cpu, "\t\tConverged!")
                break
            window = new_window
            if window > 100000000:
                self.print_cpu_analysis(cpu, "\t\tlatency higher than 100ms, Did not converge")
                self.print_latency(cpu, 0, "Sporadic")
                return

        latency = window
        self.print_latency(cpu, latency, "Sporadic")
        return latency, "Sporadic"

    def __max_exec_in_a_windown(self, vector, window):
        """
        Maximum interference an interrupt can add to a given window.
        """
        max_exec=0
        stack={}
        last_arrival=0
        for entry in vector:
            # add the new entry in the stack
            arrival = entry[0]
            exec_time = entry[1]
            stack[arrival]=exec_time

            # for each occurrence in the stack, remove those that
            # arrived before window units of time ago.
            for item in sorted(stack.keys()):
                delta = arrival - item
                if delta > window:
                    stack.pop(item)

            curr_exec = 0
            for item in stack.keys():
                curr_exec += stack[item]

            if curr_exec > max_exec:
                max_exec = curr_exec

        return max_exec

    def sliding_window_interrupt(self, cpu, ifl):
        """
        Return the latency considering the sliding window.
        """
        self.print_cpu_analysis(cpu, "\tConsidering the worst interrupt burst (sliding window):" )

        # Saved results for each int entry:
        saved_int_interference={}

        # The ifl is the initial window.
        window=ifl
        self.print_cpu_analysis(cpu, "\t\tWindow: " + str(window))

        # Get the IRQ vector list of a CPU.
        vector_list=self.get_irq_vectors_of_a_cpu(cpu)

        # Cache all the IRQ entries of an CPU. This improves the speed, while
        # consuming memory. It can be avoided by consulting the database
        # anytime a vector is tried, but this turns the analysis slower.
        #
        # TODO: create an option for low memory systems to fallback.
        irq_vector_entries={}

        for vector in vector_list:
            irq_entires = self.get_irq_entries_of_a_vector(cpu, vector)
            irq_vector_entries[vector]=irq_entires

        # Cache all the NMI entries of a CPU.
        nmi_entries=self.get_nmi_entries_of_cpu(cpu)

        # First try for NMI
        vector="NMI"
        try:
            max_exec = self.__max_exec_in_a_windown(nmi_entries, window)
        except:
            max_exec = 0

        self.print_cpu_analysis(cpu, "\t\t %9s:%9d" % (vector, max_exec))

        # Save the NMI disturbance.
        saved_int_interference[vector] = max_exec

        # Increase the window:
        window += max_exec

        # Try each IRQ
        for vector in vector_list:
            max_exec = self.__max_exec_in_a_windown(irq_vector_entries[vector], window)
            self.print_cpu_analysis(cpu, "\t\t %9d:%9d" % (vector, max_exec))
            saved_int_interference[vector] = max_exec

            # Increase the window already, to try to save time.
            window += max_exec

        self.print_cpu_analysis(cpu, "\t\tWindow: " + str(window))

        # Loop until the new_window == window. IOW: when the worst window is
        # found.
        new_window = window
        while True:

            # NMI first
            vector="NMI"
            try:
                max_exec = self.__max_exec_in_a_windown(nmi_entries, new_window - saved_int_interference[vector])
            except:
                max_exec = 0

            # If worse interference is found, update the results, and add it to
            # the new_window.
            if (max_exec > saved_int_interference[vector]):
                self.print_cpu_analysis(cpu, "\t\t %9s:%9d <- new!" % (vector, max_exec))
                new_window -= saved_int_interference[vector]
                new_window += max_exec
                saved_int_interference[vector]=max_exec

            # Same for IRQs.
            for vector in vector_list:
                max_exec = self.__max_exec_in_a_windown(irq_vector_entries[vector], new_window - saved_int_interference[vector])
                if (max_exec > saved_int_interference[vector]):
                    self.print_cpu_analysis(cpu, "\t\t %9d:%9d <- new!" % (vector, max_exec))
                    new_window -= saved_int_interference[vector]
                    new_window += max_exec
                    saved_int_interference[vector]=max_exec

            if new_window == window:
                self.print_cpu_analysis(cpu, "\t\tConverged!")
                break

            window = new_window
            self.print_cpu_analysis(cpu, "\t\tWindow: " + str(window))

        self.print_latency(cpu, window, "Sliding Window")

        return window, "Sliding Window"

    def __max_exec_in_a_windown_owcet(self, vector, window, owcet):
        """
        Maximum interference an interrupt can add to a given window considering oWCET.
        """
        max_exec=0
        stack={}
        last_arrival=0
        for entry in vector:
            # Add the new entry in the stack.
            arrival = entry[0]
            exec_time = entry[1]
            stack[arrival]=exec_time

            # For each occurrence in the stack, remove those that arrived before
            # window units of time ago.
            for item in sorted(stack.keys()):
                delta = arrival - item
                if delta > window:
                    stack.pop(item)

            curr_exec = 0
            for item in stack.keys():
                curr_exec += owcet

            if curr_exec > max_exec:
                max_exec = curr_exec

        return max_exec

    def sliding_window_interrupt_owcet(self, cpu, ifl):
        """
        return the latency considering the sliding window using the
        worst case execution time of each interrupt.
        """
        self.print_cpu_analysis(cpu, "\tConsidering the worst interrupt burst (sliding window) and oWCET:" )
        saved_int_interference={}
        window=ifl

        # get the IRQ vector list of a CPU
        vector_list=self.get_irq_vectors_of_a_cpu(cpu)

        # Cache all the IRQ entries of an CPU. This improves the speed, while
        # consuming memory. It can be avoided by consulting the database
        # anytime a vector is tried, but this turns the analysis slower.
        #
        # TODO: create an option for low memory systems to fallback.
        irq_vector_entries={}

        for vector in vector_list:
            irq_entires = self.get_irq_entries_of_a_vector(cpu, vector)
            irq_vector_entries[vector]=irq_entires

        # Cache all the NMI entries of a CPU.
        nmi_entries=self.get_nmi_entries_of_cpu(cpu)

        # Cache the oWCET of each interrupt.
        irq_owcet={}
        try:
            irq_owcet["NMI"] = self.get_nmi_cpu_owcet(cpu)
        except:
            irq_owcet["NMI"] = 0

        for vector in vector_list:
            irq_owcet[vector] = self.get_irq_vector_owcet(cpu, vector)

        self.print_cpu_analysis(cpu, "\t\tWindow: " + str(window))

        # NMI
        vector="NMI"
        try:
            max_exec = self.__max_exec_in_a_windown_owcet(nmi_entries, window, irq_owcet[vector])
        except:
            max_exec = 0

        self.print_cpu_analysis(cpu, "\t\t %9s:%9d" % (vector, max_exec))
        saved_int_interference[vector] = max_exec

        window += max_exec

        for vector in vector_list:
            max_exec = self.__max_exec_in_a_windown_owcet(irq_vector_entries[vector], window, irq_owcet[vector])
            self.print_cpu_analysis(cpu, "\t\t %9d:%9d" % (vector, max_exec))
            saved_int_interference[vector] = max_exec
            window += max_exec

        self.print_cpu_analysis(cpu, "\t\tWindow: " + str(window))

        new_window = window
        while True:

            # NMI first.
            vector="NMI"
            try:
                max_exec = self.__max_exec_in_a_windown_owcet(nmi_entries, new_window - saved_int_interference[vector], irq_owcet[vector])
            except:
                max_exec = 0

            if (max_exec > saved_int_interference[vector]):
                self.print_cpu_analysis(cpu, "\t\t %9s:%9d <- new!" % (vector, max_exec))
                new_window -= saved_int_interference[vector]
                new_window += max_exec
                saved_int_interference[vector]=max_exec

            # Then IRQ.
            for vector in vector_list:
                max_exec = self.__max_exec_in_a_windown_owcet(irq_vector_entries[vector], new_window - saved_int_interference[vector], irq_owcet[vector])
                if (max_exec > saved_int_interference[vector]):
                    self.print_cpu_analysis(cpu, "\t\t %9d:%9d <- new!" % (vector, max_exec))
                    new_window -= saved_int_interference[vector]
                    new_window += max_exec
                    saved_int_interference[vector]=max_exec

            if new_window == window:
                self.print_cpu_analysis(cpu, "\t\tConverged!")
                break

            window = new_window
            self.print_cpu_analysis(cpu, "\t\tWindow: " + str(window))

        self.print_latency(cpu, window, "Sliding Window with oWCET")

        return window, "Sliding Window with oWCET"

class RtslCPUAnalysis(RtslINTAnalysis):
    """
    This is class extends the RtslINTAnalysis to provide
    a (single) CPU view of the analysis.
    """

    __irq_analysis={ "cyclictest" : False, "none" : False, "single": False, "each" : False, "sporadic" : False, "s_window" : False, "s_window_owcet": False }

    def __init__(self, database_file="trace.rtsl", irq_analysis=[]):
        """
        Connect to the database.
        """
        super().__init__(database_file)

        # If no analysis is set, run the sliding window.
        if len(irq_analysis) == 0:
            self.enable_irq_analysis("s_window")
        else:
            for method in irq_analysis:
                self.enable_irq_analysis(method)

    def __process_trace_cpu(self, cpu):
        """
        Process the trace of a CPU.
        """
        self.print_cpu_analysis(cpu, "CPU: %3d" % cpu)

        poid=self.get_poid(cpu)
        paie=self.get_paie(cpu)
        psd=self.get_psd(cpu)
        dst=self.get_dst(cpu)

        ifl = self.interference_free_latency(cpu, poid, paie, psd, dst)

        if self.__irq_analysis["cyclictest"]:
            if self.has_cyclictest_results:
                latency, case = self.print_cyclictest(cpu)

        if self.__irq_analysis["none"]:
            self.no_interrupt(cpu, ifl)

        if self.__irq_analysis["single"]:
            self.single_interrupt(cpu, ifl)

        if self.__irq_analysis["each"]:
            self.single_of_each_interrupt(cpu, ifl)

        if self.__irq_analysis["sporadic"]:
            self.sporadic_interrupt(cpu, ifl)

        if self.__irq_analysis["s_window"]:
            self.sliding_window_interrupt(cpu, ifl)

        if self.__irq_analysis["s_window_owcet"]:
            self.sliding_window_interrupt_owcet(cpu, ifl)

    def enable_irq_analysis(self, analysis):

        try:
            self.__irq_analysis[analysis]=True
        except:
            raise Exception("The irq analysis %s does not exist" % analysis)

    def process_trace(self, cyclictest="cyclictest.txt"):
        """
        Process the trace of all CPUs of a database.
        """
        cpus = self.get_cpu_list()
        cpus.sort()

        self.has_cyclictest_results = self.open_cyclictest(cyclictest)

        for cpu in cpus:
            self.__process_trace_cpu(cpu)

class RtslAnalysis:

    __db_list=[]
    __cpu_list=[]

    bar_colors=[ '#142459', '#7d3ac1', '#de542c', '#1ac9e6', '#ea7369', '#e7e34e', '#c7f9ee' ]

    __irq_analysis_list=[]

    def __init__(self, rtsl_working_dir="rtsl_data", irq_analysis=[]):

        # This is a dictionary of cpu = file. It is used to
        # sort the __db_list by the CPU index.
        files_dict={}

        # Read the files in the working dir:
        for (dirpath, dirnames, filenames) in os.walk(rtsl_working_dir):
            for file in filenames:

                # Check if it is a .rtsl database
                if file[-5:] == ".rtsl":

                    # Get the CPU id from the "trace_ID_.rtsl" file
                    cpu_id=file[6:-5]

                    # Append to the dictionary of files indexed by the CPU
                    files_dict[cpu_id]=file

        if len(files_dict) == 0:
            raise Exception("There is no rtsl database, did you run rtsl report?")

        # get the list of CPUs
        self.__cpu_list=list(files_dict.keys())

        # sort them
        self.__cpu_list.sort()

        # fill the databased list sorted by the CPU index
        for cpu in self.__cpu_list:
            db_path="%s/%s" % (rtsl_working_dir, files_dict[cpu])
            self.__db_list.append(db_path)

    def enable_irq_analysis(self, method):
        self.__irq_analysis_list.append(method)

    def analysis_thread(self, database, io_queue):
        """
        Analyze a database file, placing the output into a queue.
        """

        analysis=RtslCPUAnalysis(database, irq_analysis=self.__irq_analysis_list)
        analysis.irq_sliding_window=True
        analysis.process_trace()
        io_queue.put(analysis.std_output)

    def run(self):
        """
        Analyze a list of database files.
        """
        import multiprocessing
        process_list=[]
        queue_list={}

        for database in self.__db_list:
            queue = multiprocessing.Queue()
            process = multiprocessing.Process(target=self.analysis_thread, args=[database, queue])
            process.start()

            process_list.append((process, queue))

        for process, queue in process_list:
            std_out=queue.get()
            process.join()

    def print_header(self):
        print("  ==== Latency Analysis! ==== ")
        print("\tTime unit is nanoseconds")
        print("\tpoid  = Preemption or Interrupt disabled [ not to schedule ] window")
        print("\tpaie  = Preemption and Interrupts enabled")
        print("\tpsd   = Preemption disabled to schedule window")
        print("\tdst   = Delay of scheduling tail")
        print("\tifl   = Interference free latency")
        print("\tINT   = Interrupts")
        print("\tIRQ   = Maskable interrupts")
        print("\tNMI   = Non-maskable interrupts")
        print("\toWCET = Observed Worst Case Execution Time")
        print("\toMIAT = Observed Minimun Inter-arrival Time")
        print("")

    def print_results(self):
        self.print_header()

        for database in self.__db_list:
            report=RtslDatabase(database)

            for line in report.get_analysis_output():
                print(line)

    # -------------------- Plot --------------------
    def __xticks_format_labels(self, cpus, results, xticks_step):
        xticks_labels=[]
        xticks_position=[]

        if cpus.__len__() == 1:
            return [], [];

        i=0
        for case in results:
            for cpu in results[case]:
                i+=1
                xticks_position.append(i)
                # print only the steps
                if (cpu % xticks_step) == 0:
                    xticks_labels.append(cpu)
                else:
                    xticks_labels.append('')

        return xticks_labels, xticks_position;

    def __plot_fill_results(self):
        plot_results={}

        for file in self.__db_list:
            database=RtslDatabase(file)

            for result in database.get_results():
                cpu=result[0]
                case=result[1]
                latency=result[2]

                try:
                    results = plot_results[case]
                except:
                    plot_results[case]={}
                    results = plot_results[case]

                results[cpu]=latency

        return plot_results

    # examples from the paper:
    # plot_results()
    # plot_results(output_file="output_compact.svg", hsize=7, vsize=4, title="", ymax=60 , ylabel="", xlabel="", legend=False, xtick_distance=4)
    # plot_results(output_file="output_ultra_compact.svg", hsize=4, vsize=4, title="", ymax=60 , ylabel="", xlabel="", legend=False, xtick_distance=0)

    def plot_results(self, output_file="rtsl_results.svg", hsize=16, vsize=7, title="Latency analysys", ymax=0, ylabel="Latency (microseconds)", xlabel="CPU", legend=True, legend_loc="upper center", legend_ncol=8, xtick_distance=1):

        try:
            import matplotlib
            import matplotlib.pyplot as plt
            import numpy as np
        except:
            raise Exception("Cannot use matplotlib or numpy")

        number_of_cpus=len(self.__cpu_list)

        results = self.__plot_fill_results()

        cpus = self.__cpu_list

        fig = plt.figure(figsize=(hsize,vsize), frameon=False)
        ax = fig.add_subplot()

        x = np.arange(len(cpus))
        width = 0.90

        tick_distance=1
        color_index=0

        for case in results:
            results_case=[]
            for cpu in results[case]:
                latency=results[case][cpu]/1000
                results_case.append(latency)
            ax.bar((x)+tick_distance, results_case, width, label=case, color=self.bar_colors[color_index])
            tick_distance+=cpus.__len__()
            color_index += 1

        if title.__len__() != 0:
            ax.set_title(title)
        if ylabel.__len__() != 0:
            ax.set_ylabel(ylabel)

        if xlabel.__len__() != 0:
            ax.set_xlabel(xlabel)

        if ymax != 0:
            ax.set_ylim(0, ymax)

        if xtick_distance == 0:
            ax.set_xticks([])
            ax.set_xticklabels([])
        else:
            xticks_labels, xticks_position = self.__xticks_format_labels(cpus, results, xtick_distance)
            ax.set_xticks(xticks_position)
            ax.set_xticklabels(xticks_labels)

        if legend:
            ax.legend(loc=legend_loc, ncol=legend_ncol)

        fig.tight_layout()
        plt.savefig(output_file)

class RtslThreadVarStats:
    __vars={ "poid" : 1, "paie" : 2, "psd": 3, "dst" : 4 }
    __var=""

    __cpus=0
    __max_value=0
    __results_micro={}

    __bpf_program=None

    __base_program="""
    BPF_PERCPU_ARRAY(micro, u64, 1001);

    TRACEPOINT_PROBE(rtsl, VAR) {
            u64 value = args->value;
            u64 *entry, new;
            int idx;

            if (value < 1000000) {
                /*
                 * This is an upper bound that means within a given
                 * microsecond. So increase 999 ns, to fit in the next
                 * bucket, unless it is an rounded number.
                 */
                value += 999;
                idx = value/1000;

                entry = micro.lookup(&idx);
                if (entry) {
                    new = *entry + 1;
                    micro.update(&idx, &new);
                }
            } else {
                /*
                 * A millisecond value is not expected on the RT.
                 * At least not in a correct environment.
                 * In this case, put in the last bucket a sing that
                 * there is something really wrong.
                 */
                idx = 1001;
                entry = micro.lookup(&idx);
                if (entry) {
                    new = *entry + 1;
                    micro.update(&idx, &new);
                }
            }

            return 0;
    }
    """

    __program=""

    def __init__(self, var="poid"):
        global BPF
        global sleep

        from time import sleep
        from bcc import BPF

        if self.__vars.get(var) == None:
            raise Exception("Unknown mode: %s" % mode)
        self.__var=var

        self.__program=self.__base_program.replace("VAR", var)

    def run(self):
        print("Tracing... Hit Ctrl-C to end.")
        self.__bpf_program = BPF(text=self.__program)

    def stop(self):
        event="rtsl:" + self.__var
        self.__bpf_program.detach_tracepoint(event)

    def __parse_results(self, micro):
        # Save the number of CPUs
        self.__cpus=micro.total_cpu

        self.__results_micro={}

        # initialize the values with 0
        for i in range(self.__cpus):
            self.__results_micro[i]={}
            for y in range(1001):
                self.__results_micro[i][y]=0

        for duration in micro.keys():
            # each value is a per_cpu counter of number of times a given key duration happened.
            cpu=0
            for value in micro.getvalue(duration):
                self.__results_micro[cpu][duration.value]=value
                cpu+=1

    def updata_results(self):
        micro = self.__bpf_program.get_table("micro")
        self.__parse_results(micro)

    def cpu_duration_list(self, cpu):
        duration_list=[]
        for duration in self.__results_micro[cpu]:
            duration_list.append(self.__results_micro[cpu][duration])
        return duration_list

    def duration_cpu_list(self, duration):
        cpu_list=[]
        for cpu in range(self.__cpus):
            cpu_list.append(self.__results_micro[cpu][duration])

        return cpu_list

    def print(self):

        print("y-axis = duration in us, x-axis = CPU, cell: times that a given y-duration happened on a x-CPU")

        cpus="     "
        for i in range(self.__cpus):
            cpus = "%s %9d" % (cpus, i)
        print(cpus + "        TOTAL")

        for index in range(1000):
            duration=self.duration_cpu_list(index)
            output=""
            total_count=0
            for cpu_count in duration:
                output = "%s %9d" % (output, cpu_count)
                total_count += cpu_count

            if total_count:
                print("%3d: %10s = %10d" % (index, output, total_count))

    def __get_max_and_hide_after(self, counters):
            max_idx = 0

            # Get the largest index that has entries.
            for idx in range(1001):
                if counters[idx] != 0:
                    max_idx = idx

            # change the zeros do nan so the are not displayed by
            # matplotlib
            for idx in range(max_idx+1, 1001):
                counters[idx] = float('nan')

            return max_idx, counters

    def __plot_all_in_one(self, plt, hsize=16, vsize=7, output_file="stats.svg"):
        cpus=[x for x in range(self.__cpus)]
        buckets=[x for x in range(1001)]
        x_max=0

        fig, ax = plt.subplots(figsize=(hsize,vsize), frameon=False)
        for cpu in cpus:
            counters=self.cpu_duration_list(cpu)
            max_idx, counters = self.__get_max_and_hide_after(counters)

            # If the largest is largest over all, save it.
            if x_max < max_idx:
                x_max = max_idx + 1

            label="%d (max = %d)" % (cpu, max_idx)
            ax.plot(buckets, counters, 'o', ls='-', label=label, markevery=1)

        # Truncate the x-axis with largest delay
        ax.set_xlim(1, x_max)

        ax.legend()

        fig.tight_layout()
        plt.savefig(output_file)

    def __plot_all(self, plt, hsize=16, vsize=7, output_file="stats.svg", plots_per_line=4):
        cpus=[x for x in range(self.__cpus)]
        buckets=[x for x in range(1001)]
        x_max=0

        lines = int(self.__cpus/ plots_per_line)
        if self.__cpus % plots_per_line:
            lines += 1

        fig, axs = plt.subplots(lines, plots_per_line, sharey=True, figsize=(hsize,vsize), frameon=False)

        for cpu in cpus:
            counters=self.cpu_duration_list(cpu)
            max_idx, counters = self.__get_max_and_hide_after(counters)

            # If the largest is largest over all, save it.
            if x_max < max_idx:
                x_max = max_idx + 1

            label="%d (max = %d)" % (cpu, max_idx)

            line=int(cpu / plots_per_line)
            col=int(cpu % plots_per_line)
            axs[line, col].plot(buckets, counters, 'o', ls='-', label=label, markevery=1)

        # Truncate the x-axis with largest delay (plus one)
        for cpu in cpus:
            line=int(cpu / plots_per_line)
            col=int(cpu % plots_per_line)

            axs[line, col].set_xlim(1, x_max)
            axs[line, col].set_ylim(0)
            axs[line, col].legend()

        fig.tight_layout()
        plt.savefig(output_file)

    def plot(self, hsize=16, vsize=7, merge=False, output_file="stats.svg"):
        try:
            import matplotlib.pyplot as plt
        except:
            raise Exception("Cannot use matplotlib")

        if merge:
            self.__plot_all_in_one(plt, hsize, vsize, output_file=output_file)
        else:
            self.__plot_all(plt, hsize, vsize, output_file=output_file)

class RtslStats:

    __s=None
    __poll_delay=1
    __quite=False
    __variable=""
    __module=None

    def __init__(self, variable="poid", delay=1, quite=False, duration="60"):
        self.__variable = variable;
        self.__delay=delay
        self.quite=quite
        self.duration = duration

        self.__module=RtslKernelModule()
        try:
            self.__module.rtsl_check_interface()
        except Exception as e:
            raise Exception("Cannot access RTSL interface: %s" % e.strerror)

        self.__s = RtslThreadVarStats(self.__variable)

    def __wait_quite(self):
        try:
            time.sleep(self.duration)
        except KeyboardInterrupt:
            print()
            return

    def __wait_print(self):

        loop=int(self.duration/self.__poll_delay)
        last=self.duration % self.__poll_delay

        for i in range(0, loop):
            try:
                self.__s.updata_results()
                os.system('clear')
                print("Hit ^C to stop")
                self.__s.print()
                time.sleep(self.__poll_delay)
            except KeyboardInterrupt:
                os.system('clear')
                print()
                return

        # The residual sleep, if the duration is not multiple of poll
        if last:
            time.sleep(last)

        os.system('clear')
        return

    def run(self):
        self.__module.rtsl_enable()

        self.__s.run()
        if self.__quite:
            self.__wait_quite()
        else:
            self.__wait_print()

        self.__module.rtsl_disable()

    def print_results(self):
        print("Histogram for %s"  % (self.__variable))
        self.__s.print()

    def plot_results(self, merge=False):
        self.__s.plot(merge=merge)

# -------------------- subcommand main --------------------
def duration_to_seconds(duration):
    "Convert a time input TIME<smhdw> to the value in seconds"

    # yes, from hwlatdetect... :-)
    if str(duration).isdigit():
        return int(duration)
    elif duration[-2].isalpha():
        raise RuntimeError("illegal suffix for seconds: '%s'" % duration[-2:-1])
    elif duration[-1:] == 's':
        return int(duration[0:-1])
    elif duration[-1:] == 'm':
        return int(duration[0:-1]) * 60
    elif duration[-1:] == 'h':
        return int(duration[0:-1]) * 3600
    elif duration[-1:] == 'd':
        return int(duration[0:-1]) * 86400
    elif duration[-1:] == 'w':
        return int(duration[0:-1]) * 86400 * 7
    else:
        raise RuntimeError("invalid input for seconds: '%s'" % duration)

def main_record(params):

    duration=duration_to_seconds(params.duration)

    try:
        rtsl=RtslTrace("record", duration=duration, backend=params.tracer, verbose=params.verbose)
    except Exception as e:
        print(str(e))
        sys.exit(1)

    rtsl.record()

def main_report(params):
    try:
        trace=RtslTrace("report", verbose=params.verbose)
    except Exception as e:
        print(str(e))
        sys.exit(1)

    trace.report(params)

    analysis=RtslAnalysis()

    if params.irq_none:
        analysis.enable_irq_analysis("none")

    if params.irq_worst_single:
        analysis.enable_irq_analysis("single")

    if params.irq_worst_each:
        analysis.enable_irq_analysis("each")

    if params.irq_sporadic:
        analysis.enable_irq_analysis("sporadic")

    if params.irq_s_window:
        analysis.enable_irq_analysis("s_window")

    if params.irq_s_window_owcet:
        analysis.enable_irq_analysis("s_window_owcet")

    analysis.run()
    analysis.print_results()
    if (params.plot):
        analysis.plot_results()

def main_stats(params):

    duration=duration_to_seconds(params.duration)

    try:
        rtsl=RtslStats(variable=params.var, duration=duration, quite=params.quite)
    except Exception as e:
        print(str(e))
        sys.exit(1)

    rtsl.run()
    rtsl.print_results()
    if params.plot:
        rtsl.plot_results(merge=params.merge)

if __name__ == '__main__':
    import platform
    import argparse
    import ntpath
    import sys
    import os
    import time

    parser = argparse.ArgumentParser(description='real-time scheduling latency')

    subparsers = parser.add_subparsers(dest='subcommand', required=True)

    # Sub commands
    record_parser=subparsers.add_parser("record", help="record trace")
    report_parser=subparsers.add_parser("report", help="report trace")
    stats_parser=subparsers.add_parser("stats", help="report trace")

    # Sub commands' main
    record_parser.set_defaults(func=main_record)
    report_parser.set_defaults(func=main_report)
    stats_parser.set_defaults(func=main_stats)

    # Global options
    parser.add_argument('-v', "--verbose", dest="verbose",
                        required=False, action='store_true',
                        help="print verbose output (like tracer's output)")

    # Record options
    record_parser.add_argument('-d', "--duration", dest="duration",
                        required=False, default=60,
                        help="duration of the trace <n>{smdw}")

    record_parser.add_argument("--tracer", choices=['perf', 'ftrace'], required=False,
                        default="ftrace",
                        help="define which trace subsystem to use (default ftrace)")

    # Report options
    report_parser.add_argument("--reparse", dest="reparse", action='store_true',
                         required=False,
                         default=False,
                         help="force re-parsing the trace file")

    report_parser.add_argument("--plot", dest="plot", action='store_true',
                        required=False,
                        default=False,
                        help="plot results")

    report_parser.add_argument('-N', "--irq_none", dest="irq_none", action='store_true',
                         required=False,
                         default=False,
                         help="Latency without IRQs")

    report_parser.add_argument('-W', "--irq_worst_single", dest="irq_worst_single", action='store_true',
                         required=False,
                         default=False,
                         help="Latency with a single (worst) IRQs")

    report_parser.add_argument('-E', "--irq_worst_each", dest="irq_worst_each", action='store_true',
                         required=False,
                         default=False,
                         help="Latency with a single (worst) occurrence of each IRQ")

    report_parser.add_argument('-P', "--irq_periodic", dest="irq_sporadic", action='store_true',
                         required=False,
                         default=False,
                         help="Latency with periodic/sporadic interrupts")

    report_parser.add_argument('-S', "--irq_sliding_window", dest="irq_s_window", action='store_true',
                         required=False,
                         default=False,
                         help="Latency with sliding window with the worst burst occurrence of all IRQs")

    report_parser.add_argument('-O', "--irq_sliding_window_owcet", dest="irq_s_window_owcet", action='store_true',
                         required=False,
                         default=False,
                         help="Latency with sliding window with the worst burst occurrence of all IRQs considering their oWCET")

    # Stats options
    stats_parser.add_argument('var', choices=['poid', 'paie', 'psd', 'dst'],
                        default="poid",
                        help="Variable to collect statistics")

    stats_parser.add_argument('-d', "--duration", dest="duration",
                        required=False, default=60,
                        help="duration of the trace <n>{smdw}")

    stats_parser.add_argument('-q', "--quite", dest="quite", action='store_true',
                        required=False,
                        default=False,
                        help="run silently")

    stats_parser.add_argument('-p', "--plot", dest="plot", action='store_true',
                        required=False,
                        default=False,
                        help="Plot results")

    stats_parser.add_argument("--plot-merge", dest="merge", action='store_true',
                        required=False,
                        default=False,
                        help="Plot results")

    params = parser.parse_args()

    params.func(params)
